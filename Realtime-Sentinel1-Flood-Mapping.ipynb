{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Import all the necessary libraries\"\"\"\n",
    "import os\n",
    "from os.path import join\n",
    "from os.path import join as opj\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "#Import form OST tool box\n",
    "from ost import Sentinel1_GRDBatch\n",
    "from ost.helpers import vector, raster\n",
    "\n",
    "#Sheduling Tasks\n",
    "from apscheduler.scheduler import Scheduler\n",
    "\n",
    "import rasterio\n",
    "from ost.helpers import helpers as h, raster as ras\n",
    "\n",
    "\n",
    "#Importing Flood Processing Module\n",
    "from Flood_OST_S1 import Sentinel1Flood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the main working directory\n",
    "path = \"D:\\\\2020\\\\46_phil_storm\\\\z_results\"\n",
    "\n",
    "\n",
    "#Defining the Area of interest as a point\n",
    "lat, lon =    17.76,121.72\n",
    "\n",
    "\n",
    "aoi = 'POINT ({} {})'.format(lon, lat)\n",
    "aoi = vector.latlon_to_wkt(lat, lon, buffer_meter=40000, envelope=True)\n",
    "\n",
    "#Defining the Area as a bounding box\n",
    "\"\"\"aoi = 'POLYGON((89.058055555556 24.277222222222,\\\n",
    "                       92.580277777778 24.277222222222,\\\n",
    "                       92.580277777778 23.373055555556,\\\n",
    "                       89.058055555556 23.373055555556,\\\n",
    "                       89.058055555556 24.277222222222))'\"\"\"\n",
    "\n",
    "\n",
    "# Time of interest - pre disaster\n",
    "start_pre = '2020-11-04'\n",
    "end_pre = '2020-11-05'\n",
    "\n",
    "# Activation occurrence date\n",
    "start_post = '2020-11-15'\n",
    "# End Activation date --- eg: current date\n",
    "end_post = '2020-11-16'\n",
    "\n",
    "#Required post date time formatting\n",
    "t1 = start_post\n",
    "y1=t1[0:4]\n",
    "m1=t1[5:7]\n",
    "d1=t1[-2:]\n",
    "\n",
    "date1 =m1+\"-\"+d1+\"-\"+y1\n",
    "d1 = datetime.strptime(date1,'%m-%d-%Y').date()\n",
    "\n",
    "#Create a processing directory\n",
    "project_dir_pre = join(path, 'phil_s1_idc_16', 'pre')\n",
    "project_dir_post = join(path, 'phil_s1_idc_16', 'post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creationa of GRD batch processing instances for pre and post scenarios\n",
    "s1_grd_pre = Sentinel1_GRDBatch(\n",
    "    project_dir=project_dir_pre,\n",
    "    aoi = aoi,\n",
    "    start = start_pre,\n",
    "    end = end_pre\n",
    ")\n",
    "\n",
    "\n",
    "#ARD parameter defining and Processing of the data\n",
    "#ARD parameters pre images\n",
    "s1_grd_pre.ard_parameters['single ARD']['resolution'] = 30\n",
    "s1_grd_pre.ard_parameters['single ARD']['product type'] = 'GTC-sigma0'\n",
    "s1_grd_pre.ard_parameters['single ARD']['create ls mask'] = False\n",
    "s1_grd_pre.ard_parameters['single ARD']['dem']['image resampling'] = 'BILINEAR_INTERPOLATION'\n",
    "s1_grd_pre.ard_parameters['single ARD']['dem']['dem name']= 'SRTM 3Sec'\n",
    "s1_grd_pre.ard_parameters['single ARD']['to db'] = 'True'\n",
    "s1_grd_pre.ard_parameters['single ARD']['polarisation'] = 'VV'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Purpose of below mentioned functions\n",
    "1. Tranfering .img to .tiff\n",
    "2. Download data and definition of process workflow.\n",
    "3. Search Latest Post image file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1)Function for convert .img data >> .ge0tiff    \n",
    "def ard_to_gtiff(infile, outfile, driver='GTiff', to_db=True):    \n",
    "    prefix = glob.glob(os.path.abspath(infile[:-4]) + '*data')[0]\n",
    "    if len(glob.glob(opj(prefix, '*VV*.img'))) == 1:\n",
    "        co_pol = glob.glob(opj(prefix, '*VV*.img'))[0]   \n",
    "    with rasterio.open(co_pol) as co:        \n",
    "        # get meta data\n",
    "        meta = co.meta\n",
    "        # update meta\n",
    "        meta.update(driver=driver, count=1, nodata=0)\n",
    "        with rasterio.open(outfile, 'w', **meta) as dst:\n",
    "            for i, window in co.block_windows(1):                \n",
    "            # read arrays and turn to dB (in case it isn't)\n",
    "                co_array = co.read(window=window)\n",
    "                for k, arr in [(1, co_array)]:\n",
    "                    dst.write(arr[0, ], indexes=k, window=window)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2)This function fill find the corresponding pre image to the post image and download both pre,post images                \n",
    "def process_downloaded_data():\n",
    "\n",
    "\n",
    "    #Latest Post Sentinel Image/s\n",
    "    latestDf_grd_post = iDf_grd_post[iDf_grd_post.acquisitiondate == iDf_grd_post.acquisitiondate.min()]\n",
    "                \n",
    "    #Finding the Orbital Corresponding Pre Image\n",
    "    s1_grd_pre.search()\n",
    "    iDf_grd_pre= s1_grd_pre.inventory.copy()\n",
    "    orbit_grd_pre =  iDf_grd_pre.loc[iDf_grd_pre['relativeorbit'].isin(np.array(latestDf_grd_post.relativeorbit.unique()))]\n",
    "    Df_grd_pre = orbit_grd_pre[orbit_grd_pre.acquisitiondate == orbit_grd_pre.acquisitiondate.max()]\n",
    "                        \n",
    "    #Download\n",
    "    s1_grd_pre.download(Df_grd_pre,mirror= '1')\n",
    "    s1_grd_post.download(latestDf_grd_post,mirror= '1')\n",
    "    print(' INFO: Latest scene found for {}'.format(latestDf_grd_post['acquisitiondate'].values[0]))\n",
    "    s1_grd_post.plot_inventory(latestDf_grd_post, transparency=.5)\n",
    "    print(' INFO: Pre scene found for {}'.format(Df_grd_pre['acquisitiondate'].values[0]))\n",
    "    s1_grd_pre.plot_inventory(Df_grd_pre, transparency=.5)\n",
    "    \n",
    "    #Process of the downloaded images\n",
    "    b1 = datetime.datetime.now()\n",
    "\n",
    "    s1_grd_pre.grds_to_ard(\n",
    "        inventory_df=Df_grd_pre, \n",
    "        timeseries=False, \n",
    "        timescan=False, \n",
    "        mosaic=True, \n",
    "        overwrite=True, \n",
    "        subset=None)\n",
    "    s1_grd_post.grds_to_ard(\n",
    "        inventory_df=latestDf_grd_post, \n",
    "        timeseries=False, \n",
    "        timescan=False, \n",
    "        mosaic=True, \n",
    "        overwrite=True, \n",
    "        subset=None)\n",
    "\n",
    "    b2 = datetime.datetime.now()-b1\n",
    "    print(\"Process execution time {}\".format(b2))\n",
    "    \n",
    "    #getting the data from dim outs puts. need to refine\n",
    "    orbit_post = latestDf_grd_post['relativeorbit'].values[0]\n",
    "    orbit_pre = Df_grd_pre['relativeorbit'].values[0]\n",
    "    date_post = str(latestDf_grd_post.acquisitiondate.max())\n",
    "    date_pre = str(Df_grd_pre.acquisitiondate.max())\n",
    "    #.dim data path\n",
    "    dim_post_dir = join(project_dir_post, 'processing', str(orbit_post), str(date_post)) \n",
    "    dim_pre_dir = join(project_dir_pre, 'processing', str(orbit_pre), str(date_pre)) \n",
    "    #making Tiff_Data in project folder\n",
    "    in_proj_folder= os.path.normpath(project_dir_post + os.sep + os.pardir)\n",
    "    if not os.path.exists(in_proj_folder+\"\\\\Tiff_Data\"):\n",
    "        os.makedirs(in_proj_folder+\"\\\\Tiff_Data\")\n",
    "    proj_tiff_folder=os.path.normpath(in_proj_folder+\"\\\\Tiff_Data\")\n",
    "\n",
    "    final_post_path = os.path.join(dim_post_dir + \"\\\\{}_{}.bs.data\".format(date_post,orbit_post))\n",
    "    final_pre_path = os.path.join(dim_pre_dir + \"\\\\{}_{}.bs.data\".format(date_pre,orbit_pre)) \n",
    "\n",
    "    #Converting dim to geotiff\n",
    "    ard_to_gtiff(final_pre_path,proj_tiff_folder+\"\\\\\"+\"pre.tiff\", driver='GTiff', to_db=False)\n",
    "    ard_to_gtiff(final_post_path,proj_tiff_folder+\"\\\\\"+\"post.tiff\", driver='GTiff', to_db=False)\n",
    "    \n",
    "    ## Processing Using Flood module\n",
    "    # making flood result folder in project folder\n",
    "    # in_proj_folder= os.path.normpath(project_dir_post + os.sep + os.pardir)\n",
    "    if not os.path.exists(in_proj_folder+\"\\\\Flood_Result\"):\n",
    "        os.makedirs(in_proj_folder+\"\\\\Flood_Result\")\n",
    "    proj_flood_result_folder=os.path.normpath(in_proj_folder+\"\\\\Flood_Result\")\n",
    "\n",
    "    # Data input and output\n",
    "    flood_data_input = str(proj_tiff_folder)+\"\\\\\"\n",
    "    flood_result_output = str(proj_flood_result_folder)+\"\\\\\"\n",
    "    \n",
    "    # From Flood_OST_S1 import Sentinel1Flood. This will produce flood result based on the change image threshold of -3 decibels\n",
    "    global test01\n",
    "    # Defining an object fro the global variables\n",
    "    test01 = Sentinel1Flood(flood_data_input,flood_result_output)\n",
    "    test01.change(out_change_name = \"change\")\n",
    "    test01.thresholding(threshold_value = -3, out_thresh_name = \"threshold\")\n",
    "    test01.maj_filtering(filter_size = 3, out_maj_name = \"majority\")\n",
    "    test01.ras2poly(out_poly_name = \"poly\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below-mentioned search_latest function will search Sentinel-1 data according to the defined time and geographical requirements using the OST (Open SAR toolkit) functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3)Search latest file post image\n",
    "def search_latest():\n",
    "    \n",
    "    # Making a  post directory to update the full inventory here\n",
    "    global s1_grd_post\n",
    "    s1_grd_post = Sentinel1_GRDBatch(\n",
    "    project_dir=project_dir_post,\n",
    "    aoi = aoi,\n",
    "    start = start_post,\n",
    "    end = end_post)\n",
    "        \n",
    "    # Post Parameter definiton\n",
    "    s1_grd_post.ard_parameters['single ARD']['resolution'] = 30\n",
    "    s1_grd_post.ard_parameters['single ARD']['product type'] = 'GTC-sigma0'\n",
    "    s1_grd_post.ard_parameters['single ARD']['create ls mask'] = False\n",
    "    s1_grd_post.ard_parameters['single ARD']['dem']['image resampling'] = 'BILINEAR_INTERPOLATION'\n",
    "    s1_grd_post.ard_parameters['single ARD']['dem']['dem name']= 'SRTM 3Sec'\n",
    "    s1_grd_post.ard_parameters['single ARD']['to db'] = 'True'\n",
    "    s1_grd_post.ard_parameters['single ARD']['polarisation'] = 'VV'    \n",
    "    \n",
    "    #Searching the latet post image\n",
    "    s1_grd_post.search()\n",
    "    #\\post\\inventory\n",
    "    global iDf_grd_post\n",
    "    inv_path = project_dir_post+'\\\\inventory'\n",
    "    con = os.listdir(inv_path)\n",
    "    if len(con) == 0:\n",
    "        print(\"Still seraching for data\")\n",
    "        \n",
    "    else:\n",
    "            \n",
    "        iDf_grd_post = s1_grd_post.inventory.copy()\n",
    "\n",
    "        #laset observatoin data time formatting\n",
    "        t2=iDf_grd_post.acquisitiondate.min()\n",
    "        y2=t2[0:4]\n",
    "        m2=t2[4:6]\n",
    "        d2=t2[-2:]\n",
    "\n",
    "        date2 =m2+\"-\"+d2+\"-\"+y2\n",
    "        d2=datetime.datetime.strptime(date2,'%m-%d-%Y').date()\n",
    "        #d2=datetime.strptime(date2,'%m-%d-%Y').date()\n",
    "        print(\"Checked for :\"+str(datetime.datetime.now()))\n",
    "        if (d2 >= d1):\n",
    "            #send_email1()\n",
    "\n",
    "            global process_start_time\n",
    "            process_start_time = str(datetime.datetime.now() + timedelta(seconds = 15))\n",
    "            sched11.add_date_job(my_job, process_start_time, ['text'])\n",
    "\n",
    "            sched11.start()\n",
    "\n",
    "            sched.shutdown()    \n",
    "        else:\n",
    "            print(\"Searching the data...\")\n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This below sheduler option will run the search_latest fuction for every time interval defined as hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sched = Scheduler()\n",
    "\n",
    "# Schedule job_function to be called every 10 minutes\n",
    "@sched.interval_schedule(hours= 1/10)\n",
    "def job_function():\n",
    "    search_latest()\n",
    "sched.start() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the \"search_latest function\" satisfies, \"process_downloaded_data\" function will be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sched11 = Scheduler()\n",
    "def my_job(test):\n",
    "    process_downloaded_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final result will be based on the threshold value of -3 decibels. After checking the result quality, to make changes \n",
    "further filtering can be done as follows. PS. test01 is the flood processing object created in the \n",
    "process_downloaded_data fucntion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter the flood result if needed\n",
    "#test01 = Sentinel1Flood(flood_data_input,flood_result_output) -- already made in previous steps\n",
    "#test01.change(out_change_name = \"change\") -- already made in previous steps\n",
    "#Process in the oder of (thresh - maj -- ras) then all the pre created result files will be overwritten.\n",
    "\"\"\"test01.thresholding(threshold_value = -5, out_thresh_name = \"threshold\")\n",
    "test01.maj_filtering(filter_size = 3, out_maj_name = \"majority\")\n",
    "test01.ras2poly(out_poly_name = \"poly\")\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
